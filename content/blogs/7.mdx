---
slug: 7
author: "Kenneth Wong"
date: "2024-09-12"
title: "L2 Regularization (Weight Decay)"
description: "A brief introduction to regularization and weight decay. Specifically L2 regularization and how it helps with model training"
readTime:  10
tag: ["ML", "deep learning", "regularization"]
thumbnail: "https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/gradient-descent-convex-function.png"
---

![formula](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTa_r50DjUhq4-W381f3VNULnbAplJ4nRv1rA&s)


## Quadratic MSE Approximation
![quadratic MSE approximation](https://hbcfj3ve6zsq0764.public.blob.vercel-storage.com/blog/IMG_894FA258998D-1-PiMk6RsWvSREquBc34yVEFWDc73f3X.jpg)
This analysis requires us to build a quadratic approximation of the loss we would expect using weight w w.r.t optimal weight w*.

I tried my best to reason the variables as it was not explained in the book so I would not be surprised if it's wrong. I am especially unconfident with the reasoning behind the Hessian and the 1/2

## Add Weight Regularization
![weight regularization](https://hbcfj3ve6zsq0764.public.blob.vercel-storage.com/blog/IMG_92AAD390E244-1-0MJPnep3LUqsDmPIH65zCVvNrfD58G.jpg)
After the previous step of finding the derivative w.r.t to w, we add the weight regularization and find out that the eigenvectors of the Hessian has been scaled.

## Visualization
![visualization](https://hbcfj3ve6zsq0764.public.blob.vercel-storage.com/blog/_-7n3ZX1vDcyfLgcw8HvIMeXoXNGm5Rk.jpg)
According to our previous analysis, the "weaker" eigenvalue gets scaled down, meaning that the regularization prioritizes more important eigenvectors. So from our contour graph, we see that w1 gets pulled closer to 0 whereas w2 does not get change a lot from its original value.

## Undetermined Problem
![undetermined problem](https://hbcfj3ve6zsq0764.public.blob.vercel-storage.com/blog/IMG_EFEFED4951EA-1-VLf3etPET7lFamVfeAhjHhkirqNLgZ.jpg)
L2 regularization helps with underdetermined problems where the covariance matrix is non-invertible, which is crucial for closed-form solutions for PCA or linear regression. Hence with the additional alpha term L2 reg can turn the matrix into an invertible matrix.